{
 "cells": [
  {
   "cell_type": "code",
   "id": "e4baef34b42e09d0",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from mfs_tools.library.distance_stuff import make_distance_matrix\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "reference_cifti_path = Path(\n",
    "    \"/mnt/cache/pfm_python/\"\n",
    "    \"/sub-ME01_task-rest_concatenated_and_demeaned_32k_fsLR.dtseries.nii\"\n",
    ")\n",
    "surface_files = {\n",
    "    'lh': Path(\n",
    "        \"/mnt/brunodata/open_data/ds005118/derivatives/sub-ME01/fs_LR/fsaverage_LR32k\"\n",
    "        \"/ME01.L.midthickness.32k_fs_LR.surf.gii\"\n",
    "    ),\n",
    "    'rh': Path(\n",
    "        \"/mnt/brunodata/open_data/ds005118/derivatives/sub-ME01/fs_LR/fsaverage_LR32k\"\n",
    "        \"/ME01.R.midthickness.32k_fs_LR.surf.gii\"\n",
    "    ),\n",
    "}\n",
    "save_to = \"/mnt/cache/pfm_python/\"\n",
    "wb_command_path = \"/usr/local/workbench/2.0.1/bin_linux64/wb_command\"\n",
    "work_dir = None\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We need to build a complete 85059 x 85059 distance matrix in parts:\n",
    "# A complete fsLR can have up to 32492 vertices per hemisphere, but\n",
    "# Lynch's actual data avoids filling the medial wall.\n",
    "\n",
    "#                   cortex                 subcortex\n",
    "#           left hem       right hem\n",
    "#     ðŸ­½                                                ðŸ­¾\n",
    "#       ðŸ­½    (1)     ðŸ­¾ ðŸ­½    (2)     ðŸ­¾ ðŸ­½            ðŸ­¾\n",
    "#   lh   29696 x 29696   29696 x 29716   29696 x 25647\n",
    "#       ðŸ­¼            ðŸ­¿ ðŸ­¼            ðŸ­¿ ðŸ­¼            ðŸ­¿\n",
    "#       ðŸ­½    (2)     ðŸ­¾ ðŸ­½    (1)     ðŸ­¾ ðŸ­½            ðŸ­¾\n",
    "#   rh   29716 x 29696   29716 x 29716   29716 x 25647\n",
    "#       ðŸ­¼            ðŸ­¿ ðŸ­¼            ðŸ­¿ ðŸ­¼            ðŸ­¿\n",
    "#       ðŸ­½            ðŸ­¾ ðŸ­½            ðŸ­¾ ðŸ­½            ðŸ­¾\n",
    "#   sc   25647 x 29696   25647 x 29716   25647 x 25647\n",
    "#       ðŸ­¼            ðŸ­¿ ðŸ­¼            ðŸ­¿ ðŸ­¼            ðŸ­¿\n",
    "#     ðŸ­¼                                                ðŸ­¿\n",
    "\n",
    "# If we built it all at once, it would have 7 billion 32-bit floats,\n",
    "# requiring 28GB just to hold it. "
   ],
   "id": "9bf5d88ca5497b36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "\n",
    "## Calculate left and right cortical distances\n",
    "\n",
    "----"
   ],
   "id": "d72f9652bcc841c0"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# 1. Start by either loading pre-built single-hemisphere distance matrices\n",
    "#    or building them.\n",
    "distance_matrices = dict()\n",
    "for (hemi) in ('lh', 'rh', ):\n",
    "    mat_file = Path(save_to) / f\"dist_{hemi}.npy\"\n",
    "    if mat_file.exists():\n",
    "        distance_matrices[hemi] = np.load(mat_file)\n",
    "    else:\n",
    "        distance_matrices[hemi] = make_distance_matrix(\n",
    "            reference_cifti_path,\n",
    "            surface_files[hemi],\n",
    "            save_to,\n",
    "            num_procs=12,\n",
    "            wb_command_path=wb_command_path,\n",
    "            work_dir=Path(save_to) / \"tmp\",\n",
    "        )\n",
    "        print(f\"built {distance_matrices[hemi].shape}-shaped distance matrix\")\n",
    "        np.save(mat_file, distance_matrices[hemi])\n",
    "\n",
    "py_lh = distance_matrices[\"lh\"]\n",
    "py_rh = distance_matrices[\"rh\"]\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Matlab\n",
    "- running with 5 workers, finished in 27:53\n",
    "\n",
    "Python\n",
    "- Running with num_procs == 3 finished in 19:38.5\n",
    "- Running with num_procs == 15 finished in 7:44.9\n",
    "- Running with num_procs == 12 finished in 8:32 & 8:28; 7:44 & 7:42; 7:21 & 7:16\n",
    "- Running with 5 workers, finished in 12:57 and 13:30\n"
   ],
   "id": "ebda1ad3ae5253ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the matrices from matlab\n",
    "\n",
    "# Ensure our lh and rh match the lh and rh from matlab.\n",
    "# Then ensure our combined cortical d matches, too.\n",
    "\n",
    "matlab_outdir = Path(\"/mnt/cache/ds005118_sub-ME01/pfm/\")\n",
    "lh_matlab_file = h5py.File(matlab_outdir / \"lh.mat\", 'r')\n",
    "rh_matlab_file = h5py.File(matlab_outdir / \"rh.mat\", 'r')\n",
    "ml_lh = np.array(lh_matlab_file.get('lh'), dtype=np.uint8)\n",
    "ml_rh = np.array(rh_matlab_file.get('rh'), dtype=np.uint8)\n",
    "print(f\"The lh.mat from matlab contains {ml_lh.shape} {str(ml_lh.dtype)}s.\")\n",
    "print(f\"The rh.mat from matlab contains {ml_rh.shape} {str(ml_rh.dtype)}s.\")\n"
   ],
   "id": "d44dbe0ef8b19b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# This comparison consumes about 30GB RAM, I assume because it compares floats.\n",
    "# So ensure there's memory available for it.\n",
    "if np.allclose(py_lh, ml_lh) and np.allclose(py_rh, ml_rh):\n",
    "    print(f\"The lh matrices from python and matlab are equal.\")\n",
    "else:\n",
    "    print(f\"There are mismatches between python and matlab.\")\n",
    "    print(f\"Python's top left corners:\")\n",
    "    print(np.hstack([py_lh[:6, :6], py_rh[:6, :6]]))\n",
    "    print(f\"Matlab's top left corners:\")\n",
    "    print(np.hstack([ml_lh[:6, :6], ml_rh[:6, :6]]))\n",
    "\n",
    "    # Extract just the values that differ between methods and compare them.\n",
    "    lh_eq = np.array(py_lh == ml_lh, dtype=np.bool)[np.tril_indices_from(py_lh)]\n",
    "    rh_eq = np.array(py_rh == ml_rh, dtype=np.bool)[np.tril_indices_from(py_rh)]\n",
    "    \n",
    "    different_ml_lh_vals = ml_lh[np.tril_indices_from(py_lh)][~lh_eq]\n",
    "    different_py_lh_vals = py_lh[np.tril_indices_from(py_lh)][~lh_eq]\n",
    "    different_ml_rh_vals = ml_rh[np.tril_indices_from(py_rh)][~rh_eq]\n",
    "    different_py_rh_vals = py_rh[np.tril_indices_from(py_rh)][~rh_eq]\n",
    "    \n",
    "    print(f\"Left hemi : {len(different_ml_lh_vals):,} / {len(lh_eq):,} differ.\")\n",
    "    print(f\"Right hemi: {len(different_ml_rh_vals):,} / {len(rh_eq):,} differ.\")\n",
    "\n",
    "    lh_diff_vals = pd.DataFrame({\n",
    "        \"py_lh\": np.astype(different_py_lh_vals, np.float32),\n",
    "        \"ml_lh\": np.astype(different_ml_lh_vals, np.float32),\n",
    "    })\n",
    "    rh_diff_vals = pd.DataFrame({\n",
    "        \"py_rh\": np.astype(different_py_rh_vals, np.float32),\n",
    "        \"ml_rh\": np.astype(different_ml_rh_vals, np.float32),\n",
    "    })\n",
    "    lh_diff_vals['delta'] = lh_diff_vals['py_lh'] - lh_diff_vals['ml_lh']\n",
    "    rh_diff_vals['delta'] = rh_diff_vals['py_rh'] - rh_diff_vals['ml_rh']\n",
    "    \n",
    "    print(\"The largest difference in lh is \"\n",
    "          f\"{lh_diff_vals['delta'].min():0.2f} or \"\n",
    "          f\"{lh_diff_vals['delta'].max():0.2f}\")\n",
    "    print(\"The largest difference in lh is \"\n",
    "          f\"{lh_diff_vals['delta'].min():0.2f} or \"\n",
    "          f\"{lh_diff_vals['delta'].max():0.2f}\")\n"
   ],
   "id": "552acd12031c2a34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "\n",
    "## Calculate subcortical, and subcortex to left and right cortical distances\n",
    "\n",
    "----"
   ],
   "id": "a055a27c9e43f748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get gifti coordinates to calculate Euclidean distance between them.\n",
    "import nibabel as nib\n",
    "from nibabel.affines import apply_affine\n",
    "\n",
    "\n",
    "# Extract the 3D Cartesian coordinates of all surface vertices\n",
    "lh_surface_img = nib.gifti.gifti.GiftiImage.from_filename(surface_files['lh'])\n",
    "rh_surface_img = nib.gifti.gifti.GiftiImage.from_filename(surface_files['rh'])\n",
    "surface_coordinates = np.vstack([\n",
    "    lh_surface_img.darrays[0].data, rh_surface_img.darrays[0].data,\n",
    "])\n",
    "print(\"Gifti Surface coordinates: \"\n",
    "      f\"[{lh_surface_img.darrays[0].data.shape} + {rh_surface_img.darrays[0].data.shape}]\"\n",
    "      f\" = {surface_coordinates.shape}\")\n",
    "\n",
    "# Extract the vertex indices into the mapped BOLD data\n",
    "img = nib.cifti2.Cifti2Image.from_filename(reference_cifti_path)\n",
    "brain_ax = img.header.get_axis(1)\n",
    "print(f\"Length of cifti2 brain_axis: {len(brain_ax)}\")\n",
    "anat_map = {\n",
    "    'CortexLeft': 'CIFTI_STRUCTURE_CORTEX_LEFT',\n",
    "    'CortexRight': 'CIFTI_STRUCTURE_CORTEX_RIGHT',\n",
    "}\n",
    "lh_surf_anat = lh_surface_img.darrays[0].metadata.get('AnatomicalStructurePrimary', '')\n",
    "lh_surf_idx = brain_ax[brain_ax.name == anat_map[lh_surf_anat]]\n",
    "print(f\"Just vertices in {str(type(lh_surf_idx))} {lh_surf_anat}: {len(lh_surf_idx)}\")\n",
    "rh_surf_anat = rh_surface_img.darrays[0].metadata.get('AnatomicalStructurePrimary', '')\n",
    "rh_surf_idx = brain_ax[brain_ax.name == anat_map[rh_surf_anat]]\n",
    "print(f\"Just vertices in {str(type(rh_surf_idx))} {rh_surf_anat}: {len(rh_surf_idx)}\")\n",
    "used_surf_axis = lh_surf_idx + rh_surf_idx\n",
    "used_surface_coordinates = surface_coordinates[used_surf_axis.vertex, :]\n",
    "\n",
    "# Get the subcortical voxels, too, from a volumetric grid rather than vertices.\n",
    "ctx_labels = list(anat_map.values())\n",
    "subcortical_coordinates = apply_affine(\n",
    "    brain_ax.affine,\n",
    "    brain_ax.voxel[~np.isin(brain_ax.name, ctx_labels)],\n",
    ")\n",
    "print(\"Nifti subcortical coordinates: \"\n",
    "      f\" = {subcortical_coordinates.shape}\")\n",
    "\n",
    "whole_brain_coordinates = np.vstack([\n",
    "    used_surface_coordinates, subcortical_coordinates\n",
    "])\n",
    "print(\"Whole brain coordinates: \"\n",
    "      f\" = {whole_brain_coordinates.shape}\")\n",
    "\n",
    "# Note that python's voxel locations are consistently shifted relative to\n",
    "# matlab's. Python's x values are ml+2mm, y=ml+2mm, z=ml-2mm.\n",
    "# Maybe 0-based vs 1-based indexing, then multiplied by the affine?\n",
    "# Maybe it's start of voxel vs end of voxel, not center?\n",
    "# It's all relative, so the effect is only between subcortical and cortical."
   ],
   "id": "c9a7cf0a7a18fab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now, calculate the Euclidean distances between all points\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "euclid_dist = cdist(\n",
    "    whole_brain_coordinates[(1, 2, 3, 40000, 84860, 84861), :],\n",
    "    whole_brain_coordinates[(1, 2, 3, 40000, 84860, 84861), :],\n",
    ")\n",
    "# This works! but it will generate a giant fucking matrix, blowing out\n",
    "# my memory. We'll have to do pieces at a time, then uint8 them,\n",
    "# and stitch them together after.\n",
    "\n"
   ],
   "id": "3dd071d6d478f5c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "\n",
    "## Put all cortical distances together\n",
    "\n",
    "----"
   ],
   "id": "30cc9f79c798ba5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start pasting lh-lh and rh-rh distances into a complete distance matrix\n",
    "# where anything between lh and rh is \"large\".\n",
    "# The largest uint8 is 2^8 == 256, which is big enough to get masked later.\n",
    "# [ [ lh ] [255s] ]\n",
    "# [ [255s] [ rh ] ]\n",
    "\n",
    "top_right_lh_x_rh = np.ones((py_lh.shape[0], py_rh.shape[1]), dtype=np.uint8) * 255\n",
    "bottom_left_rh_x_lh = np.ones((py_rh.shape[0], py_lh.shape[1]), dtype=np.uint8) * 255\n",
    "py_ctx = np.vstack([\n",
    "    np.hstack([py_lh, top_right_lh_x_rh, ]),\n",
    "    np.hstack([bottom_left_rh_x_lh, py_rh, ]),\n",
    "])\n",
    "np.save(Path(save_to) / \"dist_ctx.npy\", py_ctx)\n"
   ],
   "id": "74c43742bb656be6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
